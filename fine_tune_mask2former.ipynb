{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_split_kata = 'F:/embryo_dataset/train.txt'\n",
    "val_set_split_kata = 'F:/embryo_dataset/val.txt'\n",
    "with open(train_set_split_kata, 'r', encoding='utf-8') as f:\n",
    "    train_lines = f.readlines()\n",
    "\n",
    "with open(val_set_split_kata, 'r', encoding='utf-8') as f:\n",
    "    val_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a540237c-0991-4d6d-b0b5-a60af235387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_lines = [new_line.replace('/mnt/disk_2/AI03/phuc-lm/CleavageEmbryov1/combine_images/','F:/embryo_dataset/combine_images/') for new_line in train_lines]\n",
    "new_val_lines = [new_line.replace('/mnt/disk_2/AI03/phuc-lm/CleavageEmbryov1/combine_images/','F:/embryo_dataset/combine_images/') for new_line in val_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2ff8ef-d3f9-473f-82b5-e1b1b06db273",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_train.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in new_train_lines:\n",
    "        f.write(line)\n",
    "\n",
    "# Ghi lại val set\n",
    "with open('new_val.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in new_val_lines:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72a85c-b199-464a-818d-1e71508ffa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707b0dcf-5680-4068-a63b-8d87ea6a843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c378dc61-2175-421c-af29-1674af07706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"F:/embryo_dataset/combine_images/20230306_2559_6_F0_212.jpg\"\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "np_img = np.array(image)\n",
    "\n",
    "print(\"Shape:\", np_img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9afb2-a0ef-4ca5-aac5-8a64d99ddeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np_img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f41e134-82a9-4233-bc13-09a7995207aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_path = \"F:/embryo_dataset/combine_npy/20230306_2559_6_F0_212.npy\"\n",
    "mask = np.load(mask_path)\n",
    "\n",
    "print(\"Mask shape:\", mask.shape)\n",
    "print(\"Mask dtype:\", mask.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322df11a-f385-4ae1-b995-f5da1d00a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_instances = mask.shape[2]\n",
    "# Tạo ảnh tổng hợp bằng cách gán mỗi instance 1 giá trị khác nhau\n",
    "composite_mask = np.zeros_like(mask[:, :, 0], dtype=np.uint8)\n",
    "\n",
    "for i in range(n_instances):\n",
    "    composite_mask[mask[:, :, i] > 0] = i + 1  # tránh trùng số 0 (background)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(composite_mask, cmap='tab20') \n",
    "plt.title(\"All instances (color coded)\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95923081-d847-44cf-8552-9ec19fd8f2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import Mask2FormerImageProcessor\n",
    "import os\n",
    "from transformers import Mask2FormerForUniversalSegmentation, TrainingArguments, Trainer, DefaultDataCollator, AutoImageProcessor, Mask2FormerModel\n",
    "class Mask2FormerDataset(Dataset):\n",
    "    def __init__(self, image_paths, masks_dir, processor):\n",
    "        self.image_paths = image_paths\n",
    "        self.masks_dir = masks_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        print(img_path)\n",
    "        # Extract image_id (filename without extension)\n",
    "        img_id = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    \n",
    "        # Mask path từ masks_dir và img_id\n",
    "        mask_path = os.path.join(self.masks_dir, f\"{img_id}.npy\")\n",
    "\n",
    "        # Load image\n",
    "        original_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load mask\n",
    "        original_mask = np.load(mask_path)\n",
    "        \n",
    "        # Process image and mask\n",
    "        processed = self.processor(image=original_image,mask=original_mask)\n",
    "        image, segmentation_map=processed['image'],processed['mask']\n",
    "        # Remove batch dimension\n",
    "        # pixel_values = encoded[\"pixel_values\"].squeeze(0)\n",
    "        # mask_labels = encoded[\"pixel_masks\"].squeeze(0)\n",
    "\n",
    "        return original_image,original_mask,image,segmentation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9374df4-6b38-468b-8bed-4445410baca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "F:\\embryo_dataset\\embryo_env\\lib\\site-packages\\transformers\\utils\\deprecation.py:172: UserWarning: The following named arguments are not valid for `Mask2FormerImageProcessor.__init__` and were ignored: '_max_size'\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-small-coco-instance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c103e3c-e9f0-49d0-8c9d-0b0b4f0da5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_split_kata = 'F:/embryo_dataset/new_train.txt'\n",
    "val_set_split_kata = 'F:/embryo_dataset/new_val.txt'\n",
    "with open(train_set_split_kata, 'r', encoding='utf-8') as f:\n",
    "    train_lines = f.readlines()\n",
    "\n",
    "with open(val_set_split_kata, 'r', encoding='utf-8') as f:\n",
    "    val_lines = f.readlines()\n",
    "\n",
    "train_lines = [line.strip() for line in train_lines]\n",
    "val_lines = [line.strip() for line in val_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe0d657-62b7-4245-a7c3-9d0aa6a9c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Mask2FormerDataset(train_lines,'F:/embryo_dataset/combine_npy/', processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b186164-f33c-4c55-889f-145c30d1a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image,original_mask,pixel_values,mask_labels=train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0263a60-093b-4f7d-8945-73d676cb9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "961e87c1-5b67-4eb0-ae90-ae0b294f438e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (800, 800, 3)\n"
     ]
    }
   ],
   "source": [
    "img_path = \"F:/embryo_dataset/combine_images/20230306_2559_6_F0_212.jpg\"\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "np_img = np.array(image)\n",
    "\n",
    "print(\"Shape:\", np_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c04c6b1a-4a01-44e6-8c49-ef2e9b5b333b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask shape: (800, 800, 4)\n",
      "Mask dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "mask_path = \"F:/embryo_dataset/combine_npy/20230306_2559_6_F0_212.npy\"\n",
    "masks_np = np.load(mask_path)\n",
    "\n",
    "print(\"Mask shape:\", masks_np.shape)\n",
    "print(\"Mask dtype:\", masks_np.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b5daf4d-17c8-449d-b447-dab1ceaca71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import label\n",
    "\n",
    "instance_mask = np.zeros((800, 800), dtype=np.uint8)\n",
    "current_instance_id = 1\n",
    "\n",
    "for i in range(masks_np.shape[-1]):\n",
    "    # Nhận diện các thành phần liên thông trong mask hiện tại\n",
    "    labeled, num_features = label(masks_np[:, :, i] > 0)\n",
    "    \n",
    "    for obj_id in range(1, num_features + 1):\n",
    "        obj_mask = (labeled == obj_id)\n",
    "        instance_mask[obj_mask] = current_instance_id\n",
    "        current_instance_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8eb52e9-e747-412d-b1b2-865c8b05f3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 800)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks_np[:,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "742cbc5e-bdc8-490b-a897-652a7e865856",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_mask = np.zeros((800, 800), dtype=np.uint8)\n",
    "for i in range(masks_np.shape[-1]):\n",
    "    instance_mask[masks_np[:, :, i] > 0] = i + 1\n",
    "    \n",
    "\n",
    "inputs = processor(\n",
    "    images=[np_img],\n",
    "    segmentation_maps=[instance_mask],\n",
    "    input_data_format=\"channels_last\",\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f35ededf-44c0-4d68-b97c-5d9509679dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[ 0.0912,  0.1768,  0.1939,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [ 0.1083,  0.1939,  0.2111,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [ 0.1426,  0.2282,  0.2453,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          ...,\n",
       "          [ 0.2282,  0.2282,  0.2282,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [ 0.1939,  0.1939,  0.1939,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [ 0.1768,  0.1768,  0.1768,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "\n",
       "         [[ 0.2227,  0.3102,  0.3277,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [ 0.2402,  0.3277,  0.3452,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [ 0.2752,  0.3627,  0.3803,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          ...,\n",
       "          [ 0.3627,  0.3627,  0.3627,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [ 0.3277,  0.3277,  0.3277,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [ 0.3102,  0.3102,  0.3102,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "\n",
       "         [[ 0.4439,  0.5311,  0.5485,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [ 0.4614,  0.5485,  0.5659,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [ 0.4962,  0.5834,  0.6008,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          ...,\n",
       "          [ 0.5834,  0.5834,  0.5834,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [ 0.5485,  0.5485,  0.5485,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [ 0.5311,  0.5311,  0.5311,  ..., -1.8044, -1.8044, -1.8044]]]]), 'pixel_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]]), 'mask_labels': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]]), 'class_labels': [tensor([0, 1, 2, 3, 4])]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e113eca-b7fc-4d07-b2bb-6596d36be348",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['mask_labels']=mask_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4d952be-55a7-4a07-be09-83190140d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "mask_tensor = torch.from_numpy(masks_np).permute(2, 0, 1).float()  # shape (4, 800, 800)\n",
    "\n",
    "# Resize sử dụng interpolate (mode 'nearest' để giữ nguyên giá trị 0/1)\n",
    "mask_resized = F.interpolate(\n",
    "    mask_tensor.unsqueeze(0),  # Thêm batch dimension\n",
    "    size=(384, 384),\n",
    "    mode='nearest'\n",
    ").squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8c3814d0-9960-4265-ba71-7eb2b0a10553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4048a058-1f0c-46f5-9669-2eadf483808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask_tensor, title=\"Mask\"):\n",
    "    plt.imshow(mask_tensor.cpu().numpy(), cmap=\"gray\")\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7eb24-753c-4d8b-99d0-56285823e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(inputs['mask_labels'][0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c9ab6-967f-4daa-b187-c2b5d9a10a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(inputs['mask_labels'][0][3])\n",
    "plt.title(\"Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ca2d9-6de7-4d82-a2ce-09881a67eaba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bf6b11-6dbb-4b82-b9d9-a11e25eba60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_masks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61345de3-8981-4874-b66f-6bca57c2bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Mask2FormerImageProcessor\n",
    "import numpy as np\n",
    "\n",
    "# Giả sử bạn có ảnh và danh sách masks (mỗi mask là [H,W] nhị phân)\n",
    "image = np.random.rand(512, 512, 3)  # Ảnh RGB\n",
    "masks = [np.random.randint(0, 2, (512, 512)) for _ in range(10)]  # 10 cells\n",
    "\n",
    "# Chuyển đổi qua processor\n",
    "processor = Mask2FormerImageProcessor()\n",
    "inputs = processor(\n",
    "    images=image,\n",
    "    segmentation_masks=masks,  # Danh sách các masks\n",
    "    return_tensors=\"pt\",\n",
    "    input_data_format=\"channels_last\",\n",
    ")\n",
    "\n",
    "# Kết quả sẽ có:\n",
    "# - pixel_values: Ảnh chuẩn hóa\n",
    "# - pixel_mask: Mask toàn ảnh (nếu cần)\n",
    "# - mask_labels: Ground truth masks dạng tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d744f5b-f734-411f-a05e-48aa153dc1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['pixel_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c0c1c8-f40b-47f6-8a26-48d4f2e0d5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f537cf-9106-46c4-8c7c-6bdb188d1558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import Mask2FormerImageProcessor\n",
    "import os\n",
    "from transformers import Mask2FormerForUniversalSegmentation, TrainingArguments, Trainer, DefaultDataCollator, AutoImageProcessor, Mask2FormerModel\n",
    "import torch.nn.functional as F\n",
    "def load_dataset(txt_file):\n",
    "    dataset = []\n",
    "    with open(txt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            image_path = line.strip()\n",
    "            img_id = os.path.splitext(os.path.basename(image_path))[0]\n",
    "            mask_dir = 'F:/embryo_dataset/combine_npy/'\n",
    "            # Mask path từ masks_dir và img_id\n",
    "            mask_path = os.path.join(mask_dir, f\"{img_id}.npy\")\n",
    "\n",
    "            \n",
    "            try:\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                mask = np.load(mask_path, allow_pickle=True)\n",
    "                dataset.append({'image': image, 'mask': mask})\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi khi load {image_path} hoặc {mask_path}: {e}\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Load train và val\n",
    "train_data = load_dataset(\"new_train.txt\")\n",
    "val_data = load_dataset(\"new_val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d7ab221-5cbc-4ccd-b8dc-d0b6e2afc271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSegmentationDataset(Dataset):\n",
    "    \"\"\"Image segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, processor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        original_image = np.array(self.dataset[idx]['image'])\n",
    "        original_segmentation_map = np.array(self.dataset[idx]['mask'])\n",
    "        \n",
    "        inputs = processor(\n",
    "            images=[original_image],\n",
    "            input_data_format=\"channels_last\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        mask_tensor = torch.from_numpy(self.dataset[idx]['mask']).permute(2, 0, 1).float()  # shape (4, 800, 800)\n",
    "\n",
    "        # Resize sử dụng interpolate (mode 'nearest' để giữ nguyên giá trị 0/1)\n",
    "        mask_resized = F.interpolate(\n",
    "                mask_tensor.unsqueeze(0),  # Thêm batch dimension\n",
    "                size=(384, 384),\n",
    "                mode='nearest').squeeze(0)\n",
    "\n",
    "\n",
    "        mask_tensor = (mask_resized > 0).float()  # Giờ mask có giá trị 0 hoặc 1, kiểu int\n",
    "\n",
    "   \n",
    "        num_instances = mask_resized.shape[0]\n",
    "        inputs['pixel_values']=inputs['pixel_values'].squeeze(0)\n",
    "        inputs['mask_labels']=mask_tensor\n",
    "        inputs['class_labels']=[torch.ones(num_instances, dtype=torch.int64)]\n",
    "        return [inputs, original_image, original_segmentation_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5cbb44d-189c-4136-be75-3b0151e2cdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "F:\\embryo_dataset\\embryo_env\\lib\\site-packages\\transformers\\utils\\deprecation.py:172: UserWarning: The following named arguments are not valid for `Mask2FormerImageProcessor.__init__` and were ignored: '_max_size'\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-small-coco-instance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63da17fd-2cc8-4db8-8539-e9543b95c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageSegmentationDataset(train_data,processor)\n",
    "val_dataset = ImageSegmentationDataset(val_data,processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9488b97d-6b01-4bc8-b6c6-48731e034807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pixel_values': tensor([[[ 0.0912,  0.1768,  0.1939,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [ 0.1083,  0.1939,  0.2111,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [ 0.1426,  0.2282,  0.2453,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          ...,\n",
       "          [ 0.2282,  0.2282,  0.2282,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [ 0.1939,  0.1939,  0.1939,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [ 0.1768,  0.1768,  0.1768,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "         [[ 0.2227,  0.3102,  0.3277,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [ 0.2402,  0.3277,  0.3452,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [ 0.2752,  0.3627,  0.3803,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          ...,\n",
       "          [ 0.3627,  0.3627,  0.3627,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [ 0.3277,  0.3277,  0.3277,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [ 0.3102,  0.3102,  0.3102,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "         [[ 0.4439,  0.5311,  0.5485,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [ 0.4614,  0.5485,  0.5659,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [ 0.4962,  0.5834,  0.6008,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          ...,\n",
       "          [ 0.5834,  0.5834,  0.5834,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [ 0.5485,  0.5485,  0.5485,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [ 0.5311,  0.5311,  0.5311,  ..., -1.8044, -1.8044, -1.8044]]]), 'pixel_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]]]), 'mask_labels': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]), 'class_labels': [tensor([1, 1, 1, 1])]},\n",
       " array([[[127, 127, 127],\n",
       "         [129, 129, 129],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        [[127, 127, 127],\n",
       "         [129, 129, 129],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        [[127, 127, 127],\n",
       "         [130, 130, 130],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        [[134, 134, 134],\n",
       "         [134, 134, 134],\n",
       "         [134, 134, 134],\n",
       "         ...,\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        [[134, 134, 134],\n",
       "         [134, 134, 134],\n",
       "         [134, 134, 134],\n",
       "         ...,\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]]], shape=(800, 800, 3), dtype=uint8),\n",
       " array([[[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]]], shape=(800, 800, 4), dtype=uint8)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1b75637-7bc4-4251-af42-1a4b211a9fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 384, 384])\n",
      "torch.Size([8, 384, 384])\n",
      "8\n",
      "(800, 800, 3)\n",
      "(800, 800, 8)\n"
     ]
    }
   ],
   "source": [
    "inputs, original_image, original_segmentation_map = train_dataset[2]\n",
    "print(inputs['pixel_values'].shape)\n",
    "print(inputs['mask_labels'].shape)\n",
    "print(len(inputs['class_labels'][0]))\n",
    "print(original_image.shape)\n",
    "print(original_segmentation_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f630082-5dbe-447a-b584-62bb9e6b6c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item, *_ in batch])\n",
    "    \n",
    "    mask_labels = [item['mask_labels'] for item, *_ in batch]  # List[tensor(N_i, H, W)]\n",
    "    class_labels = [item['class_labels'][0] for item, *_ in batch]  # List[tensor(N_i)]\n",
    "\n",
    "    original_images = [orig_img for _, orig_img, *_ in batch]\n",
    "    original_masks = [orig_mask for *_, orig_mask in batch]\n",
    "\n",
    "    return {\n",
    "        'pixel_values': pixel_values,\n",
    "        'mask_labels': mask_labels,\n",
    "        'class_labels': class_labels,\n",
    "        'original_images': original_images,\n",
    "        'original_masks': original_masks,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb8e718b-1af8-4c72-b9a6-5317291944c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7748cb7b-1fea-4673-b8b5-c73c75f56400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([2, 3, 384, 384])\n",
      "mask_labels torch.Size([4, 384, 384])\n",
      "class_labels torch.Size([4])\n",
      "original_images (800, 800, 3)\n",
      "original_masks (800, 800, 4)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k,v.shape)\n",
    "  else:\n",
    "    print(k,v[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bd33b65-4f75-40ab-80d2-e13fbfc398ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "      shape=(3, 384, 384), dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_labels = batch[\"mask_labels\"][1].numpy()\n",
    "mask_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50ae3b12-82ce-43e6-8554-7f3a7d3184d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_instance_masks(mask_tensor):\n",
    "    \"\"\"\n",
    "    mask_tensor: numpy array hoặc torch tensor với shape (N, H, W)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    if not isinstance(mask_tensor, np.ndarray):\n",
    "        mask_tensor = mask_tensor.numpy()\n",
    "\n",
    "    N = mask_tensor.shape[0]\n",
    "    cols = 4\n",
    "    rows = (N + cols - 1) // cols\n",
    "\n",
    "    plt.figure(figsize=(cols * 3, rows * 3))\n",
    "    for i in range(N):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(mask_tensor[i], cmap='gray')\n",
    "        plt.title(f\"Instance {i}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3322fa18-20dd-47b6-8825-d0454a4b8e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_instance_masks(mask_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "305b7d1f-d33b-4e42-b991-e1c7b2fe9581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_instance_masks(mask_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66a9ab79-6363-47d5-8242-614ed72d9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Mask2FormerConfig, Mask2FormerForUniversalSegmentation\n",
    "config = Mask2FormerConfig.from_pretrained(\"facebook/mask2former-swin-small-coco-instance\")\n",
    "config.num_labels = 1  # update số class\n",
    "\n",
    "model = Mask2FormerForUniversalSegmentation(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5702e001-0ff8-4979-9c05-41487d14093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(batch[\"pixel_values\"],\n",
    "                class_labels=batch[\"class_labels\"],\n",
    "                mask_labels=batch[\"mask_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "913e9130-61e7-4812-8bd3-805908d16a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['class_queries_logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d31c8486-4130-49b4-9ce4-6864b0b28e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs['class_queries_logits']\n",
    "probs = F.softmax(logits, dim=-1)  # shape: [B, Q, 2]\n",
    "\n",
    "scores, labels = probs.max(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5855236-cfb0-48a2-9176-cb42efba4fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]]], shape=(800, 800, 4), dtype=uint8),\n",
       " array([[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]], shape=(800, 800, 3), dtype=uint8)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"original_masks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "154ecfb1-7237-47fa-9097-f314b5cc7662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99df0f93-51f8-4895-9c6e-44746016114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "293fb7e9-844b-4e9b-809b-7432dc4c6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MaskFormerImageProcessor\n",
    "\n",
    "# Create a preprocessor\n",
    "preprocessor = MaskFormerImageProcessor(ignore_index=0, reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5b06776-fae8-48d7-89fb-d1d9e4aad5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for labels in batch[\"class_labels\"]:\n",
    "    print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afe06384-27c5-4d2c-86ae-159a31cbc6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e961736c977c4aa4a7631905f33bf966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/615 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 46.15873336791992\n",
      "Loss: 29.028580882761737\n",
      "Loss: 26.89961600422266\n",
      "Loss: 25.7221657002091\n",
      "Loss: 25.097546765334588\n",
      "Loss: 24.15378639655199\n",
      "Loss: 22.88559500905321\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d7538bac9842d7bbb3cb6313f7ebab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\embryo_dataset\\embryo_env\\lib\\site-packages\\datasets\\features\\image.py:347: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Predictions and/or references don't match the expected format.\nExpected format: {'predictions': Image(mode=None, decode=True, id=None), 'references': Image(mode=None, decode=True, id=None)},\nInput predictions: [tensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), tensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')],\nInput references: [array([[[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       ...,\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]]], shape=(800, 800, 9), dtype=uint8), array([[[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       ...,\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]]], shape=(800, 800, 7), dtype=uint8)]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;66;03m# get ground truth segmentation maps\u001b[39;00m\n\u001b[0;32m     58\u001b[0m   ground_truth_segmentation_maps \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_masks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 60\u001b[0m   \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mground_truth_segmentation_maps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredicted_segmentation_maps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean IoU:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metric\u001b[38;5;241m.\u001b[39mcompute(num_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(id2label), ignore_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_iou\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mF:\\embryo_dataset\\embryo_env\\lib\\site-packages\\evaluate\\module.py:546\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    540\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    542\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m     )\n\u001b[1;32m--> 546\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Predictions and/or references don't match the expected format.\nExpected format: {'predictions': Image(mode=None, decode=True, id=None), 'references': Image(mode=None, decode=True, id=None)},\nInput predictions: [tensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), tensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')],\nInput references: [array([[[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       ...,\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]]], shape=(800, 800, 9), dtype=uint8), array([[[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       ...,\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]]], shape=(800, 800, 7), dtype=uint8)]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "running_loss = 0.0\n",
    "num_samples = 0\n",
    "for epoch in range(3):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  model.train()\n",
    "  for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "      # Reset the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(\n",
    "          pixel_values=batch[\"pixel_values\"].to(device),\n",
    "          mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "          class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "      )\n",
    "\n",
    "      # Backward propagation\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "\n",
    "      batch_size = batch[\"pixel_values\"].size(0)\n",
    "      running_loss += loss.item()\n",
    "      num_samples += batch_size\n",
    "\n",
    "      if idx % 100 == 0:\n",
    "        print(\"Loss:\", running_loss/num_samples)\n",
    "\n",
    "      # Optimization\n",
    "      optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  for idx, batch in enumerate(tqdm(test_dataloader)):\n",
    "    if idx > 5:\n",
    "      break\n",
    "\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "      outputs = model(pixel_values=pixel_values.to(device))\n",
    "\n",
    "    # get original images\n",
    "    original_images = batch[\"original_images\"]\n",
    "    target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n",
    "    # predict segmentation maps\n",
    "    predicted_segmentation_maps = preprocessor.post_process_semantic_segmentation(outputs,\n",
    "                                                                                  target_sizes=target_sizes)\n",
    "\n",
    "    # get ground truth segmentation maps\n",
    "    ground_truth_segmentation_maps = batch[\"original_masks\"]\n",
    "\n",
    "    metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "  \n",
    "\n",
    "  print(\"Mean IoU:\", metric.compute(num_labels = len(id2label), ignore_index = 0)['mean_iou'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f138da45-70e6-48f5-83a8-dd4f6315ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "44ee2e06-1723-4e7e-89f7-195a7facc6f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f7a4a48-3865-4a03-9d16-b82ddf41bdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 255], dtype=uint8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(batch[\"original_masks\"][0][:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2fdad48e-010a-4aa4-998a-888566055c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 2])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['class_queries_logits'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0545d10c-092b-47a2-8b2f-68f36b11fb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c9826b-e5a9-4801-b78b-2bde8e4d9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def prepare_target(masks_np):\n",
    "    # masks_np: numpy array (H, W, N)\n",
    "    H, W, N = masks_np.shape\n",
    "    masks = torch.as_tensor(np.moveaxis(masks_np, -1, 0), dtype=torch.uint8)  # (N, H, W)\n",
    "    \n",
    "    boxes = []\n",
    "    for i in range(N):\n",
    "        pos = np.where(masks[i].numpy())\n",
    "        xmin = np.min(pos[1])\n",
    "        xmax = np.max(pos[1])\n",
    "        ymin = np.min(pos[0])\n",
    "        ymax = np.max(pos[0])\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "    \n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    labels = torch.ones((N,), dtype=torch.int64)  # Nếu tất cả là cùng 1 class\n",
    "    \n",
    "    target = {\n",
    "        \"boxes\": boxes,\n",
    "        \"labels\": labels,\n",
    "        \"masks\": masks\n",
    "    }\n",
    "    return target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "538dda20-b3df-4bfd-be6d-89f5545c0458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\embryo_dataset\\embryo_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "F:\\embryo_dataset\\embryo_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "\n",
    "model = maskrcnn_resnet50_fpn(pretrained=False, num_classes=2)  # 1 class + background\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49f05853-7edf-4d4c-91d0-4f53f693bada",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'torchvision::nms' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchvision::nms' is only available for these backends: [CPU, Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\cpu\\nms_kernel.cpp:112 [kernel]\nMeta: registered at /dev/null:214 [kernel]\nQuantizedCPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\quantized\\cpu\\qnms_kernel.cpp:124 [kernel]\nBackendSelect: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:479 [backend fallback]\nFunctionalize: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:100 [backend fallback]\nAutogradOther: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:91 [backend fallback]\nAutogradXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:104 [backend fallback]\nAutogradLazy: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradMTIA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:95 [backend fallback]\nTracer: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\autocast\\nms_kernel.cpp:34 [kernel]\nAutocastMTIA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:466 [backend fallback]\nAutocastXPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\autocast\\nms_kernel.cpp:41 [kernel]\nAutocastMPS: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\autocast\\nms_kernel.cpp:27 [kernel]\nFuncTorchBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:208 [backend fallback]\nPythonTLSSnapshot: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:475 [backend fallback]\nPreDispatch: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:198 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(img\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[0;32m     34\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m---> 36\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mF:\\embryo_dataset\\embryo_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mF:\\embryo_dataset\\embryo_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mF:\\embryo_dataset\\embryo_env\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[1;32m--> 104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[0;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[1;32mF:\\embryo_dataset\\embryo_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mF:\\embryo_dataset\\embryo_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mF:\\embryo_dataset\\embryo_env\\lib\\site-packages\\torchvision\\models\\detection\\rpn.py:373\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[1;34m(self, images, features, targets)\u001b[0m\n\u001b[0;32m    371\u001b[0m proposals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_coder\u001b[38;5;241m.\u001b[39mdecode(pred_bbox_deltas\u001b[38;5;241m.\u001b[39mdetach(), anchors)\n\u001b[0;32m    372\u001b[0m proposals \u001b[38;5;241m=\u001b[39m proposals\u001b[38;5;241m.\u001b[39mview(num_images, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m--> 373\u001b[0m boxes, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_proposals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_anchors_per_level\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m losses \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n",
      "File \u001b[1;32mF:\\embryo_dataset\\embryo_env\\lib\\site-packages\\torchvision\\models\\detection\\rpn.py:289\u001b[0m, in \u001b[0;36mRegionProposalNetwork.filter_proposals\u001b[1;34m(self, proposals, objectness, image_shapes, num_anchors_per_level)\u001b[0m\n\u001b[0;32m    286\u001b[0m boxes, scores, lvl \u001b[38;5;241m=\u001b[39m boxes[keep], scores[keep], lvl[keep]\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# non-maximum suppression, independently done per level\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m keep \u001b[38;5;241m=\u001b[39m \u001b[43mbox_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatched_nms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnms_thresh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;66;03m# keep only topk scoring predictions\u001b[39;00m\n\u001b[0;32m    292\u001b[0m keep \u001b[38;5;241m=\u001b[39m keep[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_nms_top_n()]\n",
      "File \u001b[1;32mF:\\embryo_dataset\\embryo_env\\lib\\site-packages\\torchvision\\ops\\boxes.py:76\u001b[0m, in \u001b[0;36mbatched_nms\u001b[1;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _batched_nms_vanilla(boxes, scores, idxs, iou_threshold)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_batched_nms_coordinate_trick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\embryo_dataset\\embryo_env\\lib\\site-packages\\torch\\jit\\_trace.py:1445\u001b[0m, in \u001b[0;36m_script_if_tracing.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m   1442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m R:\n\u001b[0;32m   1443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing():\n\u001b[0;32m   1444\u001b[0m         \u001b[38;5;66;03m# Not tracing, don't do anything\u001b[39;00m\n\u001b[1;32m-> 1445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1447\u001b[0m     compiled_fn: Callable[P, R] \u001b[38;5;241m=\u001b[39m script(wrapper\u001b[38;5;241m.\u001b[39m__original_fn)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mF:\\embryo_dataset\\embryo_env\\lib\\site-packages\\torchvision\\ops\\boxes.py:95\u001b[0m, in \u001b[0;36m_batched_nms_coordinate_trick\u001b[1;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[0;32m     93\u001b[0m offsets \u001b[38;5;241m=\u001b[39m idxs\u001b[38;5;241m.\u001b[39mto(boxes) \u001b[38;5;241m*\u001b[39m (max_coordinate \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(boxes))\n\u001b[0;32m     94\u001b[0m boxes_for_nms \u001b[38;5;241m=\u001b[39m boxes \u001b[38;5;241m+\u001b[39m offsets[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m---> 95\u001b[0m keep \u001b[38;5;241m=\u001b[39m \u001b[43mnms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes_for_nms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keep\n",
      "File \u001b[1;32mF:\\embryo_dataset\\embryo_env\\lib\\site-packages\\torchvision\\ops\\boxes.py:41\u001b[0m, in \u001b[0;36mnms\u001b[1;34m(boxes, scores, iou_threshold)\u001b[0m\n\u001b[0;32m     39\u001b[0m     _log_api_usage_once(nms)\n\u001b[0;32m     40\u001b[0m _assert_has_ops()\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\embryo_dataset\\embryo_env\\lib\\site-packages\\torch\\_ops.py:1158\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[0;32m   1157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Could not run 'torchvision::nms' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchvision::nms' is only available for these backends: [CPU, Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\cpu\\nms_kernel.cpp:112 [kernel]\nMeta: registered at /dev/null:214 [kernel]\nQuantizedCPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\quantized\\cpu\\qnms_kernel.cpp:124 [kernel]\nBackendSelect: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:479 [backend fallback]\nFunctionalize: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:100 [backend fallback]\nAutogradOther: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:91 [backend fallback]\nAutogradXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:104 [backend fallback]\nAutogradLazy: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradMTIA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:95 [backend fallback]\nTracer: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\autocast\\nms_kernel.cpp:34 [kernel]\nAutocastMTIA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:466 [backend fallback]\nAutocastXPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\autocast\\nms_kernel.cpp:41 [kernel]\nAutocastMPS: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\autocast\\nms_kernel.cpp:27 [kernel]\nFuncTorchBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:208 [backend fallback]\nPythonTLSSnapshot: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:475 [backend fallback]\nPreDispatch: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:198 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MySegmentationDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.dataset[idx]['image']\n",
    "        if isinstance(img, Image.Image):  # kiểm tra nếu là PIL Image\n",
    "            img = np.array(img)  # chuyển sang numpy array\n",
    "        img = torch.as_tensor(img, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "        target = prepare_target(self.dataset[idx]['mask'])\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# Khởi tạo dataset\n",
    "dataset = MySegmentationDataset(train_data)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "model = model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    for images, targets in dataloader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f3edf80-eff1-44e3-a209-7c8ae7e5b1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.6039, 0.6039, 0.6039,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6039, 0.6000, 0.5961,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6000, 0.5922, 0.5804,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.6196, 0.6235, 0.6235,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6196, 0.6196, 0.6235,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6157, 0.6196, 0.6235,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.6039, 0.6039, 0.6039,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6039, 0.6000, 0.5961,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6000, 0.5922, 0.5804,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.6196, 0.6235, 0.6235,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6196, 0.6196, 0.6235,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6157, 0.6196, 0.6235,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.6039, 0.6039, 0.6039,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6039, 0.6000, 0.5961,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6000, 0.5922, 0.5804,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.6196, 0.6235, 0.6235,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6196, 0.6196, 0.6235,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6157, 0.6196, 0.6235,  ..., 0.0000, 0.0000, 0.0000]]]),\n",
       " {'boxes': tensor([[480., 191., 672., 360.],\n",
       "          [622., 242., 768., 411.],\n",
       "          [502., 223., 660., 378.],\n",
       "          [577., 304., 747., 465.],\n",
       "          [404., 256., 559., 405.],\n",
       "          [404., 304., 601., 492.],\n",
       "          [409., 391., 588., 536.],\n",
       "          [523., 381., 720., 582.],\n",
       "          [482., 331., 660., 525.]]),\n",
       "  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2304b7-72c5-42aa-b095-869bc37de504",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0c13be3-d83b-41d8-9d93-1686086d5b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.4980, 0.5098, 0.5176,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.4980, 0.5098, 0.5176,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.4980, 0.5098, 0.5176,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.5294, 0.5373, 0.5451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.5333, 0.5373, 0.5451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.5333, 0.5412, 0.5490,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.4980, 0.5098, 0.5176,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.4980, 0.5098, 0.5176,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.4980, 0.5098, 0.5176,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.5294, 0.5373, 0.5451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.5333, 0.5373, 0.5451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.5333, 0.5412, 0.5490,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.4980, 0.5098, 0.5176,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.4980, 0.5098, 0.5176,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.4980, 0.5098, 0.5176,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.5294, 0.5373, 0.5451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.5333, 0.5373, 0.5451,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.5333, 0.5412, 0.5490,  ..., 0.0000, 0.0000, 0.0000]]]),)\n",
      "({'boxes': tensor([[264., 300., 484., 471.],\n",
      "        [194., 396., 404., 604.],\n",
      "        [215., 431., 401., 632.],\n",
      "        [343., 445., 529., 647.]]), 'labels': tensor([1, 1, 1, 1]), 'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)},)\n"
     ]
    }
   ],
   "source": [
    "for images, targets in dataloader:\n",
    "    print(images)\n",
    "    print(targets)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334cc69c-c79b-4f03-a718-5a07fc016185",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torchvision\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"torchvision:\", torchvision.__version__)\n",
    "print(\"CUDA:\", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac4140fa-633d-428f-9ba7-4652f34b4c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa1f8af-d091-424b-9ff9-ff307b59c210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
